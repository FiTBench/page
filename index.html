<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">

  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>FiTBench</title>

  <link rel="icon" type="image/png" href="static/images/fig_icon.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }

    .highlight-box {
    background-color: #e6f1f8;
    border-radius: 12px;
    padding: 1rem 1.5rem;
    margin: 1rem 0;
    font-size: 20px;
    font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  }

  .highlight-box b {
    font-weight: bold;
  }

  .finding-box {
        background-color: #f8f9fa;
        border-radius: 20px;
        padding: 1.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
      }
  
      .finding-content {
        margin: 0;
      }
  
      .number {
        font-weight: bold;
        color: #2c3e50;
      }
  
      .arrow {
        color: #7f8c8d;
        margin: 0 0.5rem;
        font-weight: bold;
      }
  
      .conclusion {
        color: #2980b9;
        font-weight: bold;
      }
  
      .text-center {
        text-align: center;
      }

      .list1 {
      list-style-type: circle;
      }

      .list2 {
      list-style-type: square;
      margin-left: 2em;
      }
</style>
  
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div style="display: flex; align-items: center; justify-content: center;">
              <img src="static/images/fig_icon.png" style="height: 60px; margin-right: 10px; margin-top: -10px;">
              <h1 class="title is-1 publication-title">FiTBench</h1>
            </div>
            <h3><b>
              Benchmark for Scene Graph Anticipation with Fine-grained Text Cues
            </b></h3>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Anonymous Author(s)</a><sup>*</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Submission ID : 88<br>ACM MM’25, October 27–31,2025, Dublin, Lreland</span>
                  </div>
 
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/FiTBench_supp.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <!-- <i class="fas fa-file-pdf"></i>  -->
                        <img src="static/images/Supplementart_Material.png" style="width: 20px; height: 20px;">
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
                  
                  <!-- Github link -->
                  <!--
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="figs/hf-logo.png" style="width: 20px; height: 20px;">
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                -->

                <!-- Datasets link -->
                  <span class="link-block">
                    <a href="https://drive.google.com/drive/folders/1S315-nedVQTkJoCXEAljAYRYguKPpsCF?usp=sharing" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/Datasets.png" style="width: 20px; height: 20px;">
                    </span>
                    <span>Datasets</span>
                  </a>
                </span>
                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
 <section class="section" style="background-color:#efeff081">
   <div class="container is-max-desktop">
     <div class="columns is-centered has-text-centered">
       <div class="column is-six-fifths">
         <h2 class="title is-3">Abstract</h2>
         <div class="content has-text-justified">
          <p>
            Scene graph anticipation (SGA) aims to predict relations among objects represented as a scene graph for unseen future based only on the past visual observations. As an extremely challenging task, SGA requires fine-grained narrative reasoning capability to understand complex object interactions and their dynamics over time. However, existing datasets lack annotations for contextual drivers of relation evolution (e.g., person emotions, object states), thus failing to support such reasoning ability. To address these limitations, we present FiTBench, a benchmark comprising two datasets with comprehensive semantics and diverse scenarios. First, by providing a systematic annotation pipeline, FiTBench details cues of scene and narrative evolution thus enabling fine-grained reasoning. Second, FiTBench integrates ego-centric videos with rare hand-object interactions, enriching existing benchmarks and challenging current SGA methods. Third, a model-agnostic Text-Augmented Visual Semantics(TAVS) module is proposed to reason about narrative evolution by incorporating fine-grained text cues. Experimental results show that by integrating TAVS with fine-grained text cues, six representative SGA models achieve average gains of 18% and 26% on two FiTBench datasets, respectively.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Content blocks with image and text -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="text-align: center;">
        FiTBench
      </h2>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        FiTBench supports the required fine-grained narrative reasoning by detailing contextual drivers and scene evolution cues.
      </p>
      <figure class="image" style="margin-bottom: 1.5rem;">
        <img src="static/images/fig_motivation.png" alt="fig_motivation" style="max-width: 100%; height: auto;">
      </figure>
      <h3 class="title is-4">
        Fine-grained text cues
      </h3>
      <ul style="margin-left: 1.5em; list-style-type: disc; margin-top: 1.5rem; margin-bottom: 1.5rem;">
        <li style="font-size: 20px;">
          <strong style="color: #99CC00;">Emotion annotation</strong> describes the subject's emotional state in each video frame.
        </li>
        <li style="font-size: 20px;">
          <strong style="color: #4BACC6;">State annotation</strong> is used to capture changes in the functional and physical properties of objects in a video.
        </li>
        <li style="font-size: 20px;">
          <strong style="color: #C05046;">Frame-level caption annotation</strong> provide diverse contextual information by describing each video frame with text.
        </li>
      </ul>
      <h3 class="title is-4">
        Ego-centirc
      </h3>
      <ul style="margin-left: 1.5em; list-style-type: disc; margin-top: 1.5rem; margin-bottom: 1.5rem;">
        <li style="font-size: 20px;">
          <strong style="color: #F59D56;">Ego-centric views</strong> emphasize global visual relationship, while <strong style="color: #7B619C;">ego-centric views</strong> are dominated by hand-operated. This distribution shift brings new challenges to scene graph anticipation in ego-centric views.
        </li>
      </ul>

      <h2 class="title is-3" style="text-align: center;">
        Benchmark Construction
      </h2>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        To obtain fine-grained text cues, we design a systematic annotation pipeline based on the large-scale vision-language model <strong style="color:#6666FF">Qwen2.5VL-7B</strong>. 
      </p>
      <figure class="image" style="margin-bottom: 1.5rem;">
        <img src="static/images/fig_page_pipeline.png" alt="fig_pipeline" style="max-width: 100%; height: auto;">
      </figure>
      <ul style="margin-left: 1.5em; list-style-type: disc; margin-top: 1.5rem; margin-bottom: 1.5rem;">
        <li style="font-size: 20px;">
          <strong style="color: #99CC00;">Emotion</strong> and <strong style="color: #4BACC6;">state</strong> annotations are generated using Qwen2.5VL-7B with single-word prompts. To ensure quality, we filter abnormal outputs, merge semantically similar labels, and remove noisy or meaningless ones.
        </li>
        <li style="font-size: 20px;">
          <strong style="color: #C05046;">Frame-level caption</strong> in <strong style="color: #f7a049;">FiT-AG</strong> dataset are generated via <strong style="color:#6666FF">Qwen2.5VL-7B</strong>’s image description capability.
        </li>
      </ul>

      <h2 class="title is-3" style="text-align: center;">
        Benchmark Statistic
      </h2>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        FiTBench includes two subsets, <strong style="color: #f7a049;">FiT-AG</strong> and <strong style="color: #b8aeea;">FiT-PVSG</strong>, derived from the Action Genome and OpenPVSG datasets.
      </p>

      <h3 class="title is-4">
        Emotion Statistic
      </h3>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        We annotated <strong style="color: #f7a049;">53</strong> and <strong style="color: #b8aeea;">54</strong> emotion categories in the <strong style="color: #f7a049;">FiT-AG</strong> and <strong style="color: #b8aeea;">FiT-PVSG</strong> datasets, respectively.
        Based on subject emotion characteristics, we categorized emotion labels into seven categories: 
      </p>
      <figure class="image" style="margin-bottom: 1.5rem;">
        <img src="static/images/fig_emotion_distribution.png" alt="fig_emotion" style="max-width: 100%; height: auto;">
      </figure>
      <!--
      <ul style="margin-left: 1.5em; list-style-type: disc; margin-top: 1.5rem; margin-bottom: 1.5rem;">
        <li style="font-size: 20px;">
          Neutral Emotions: A calm, emotionally stable state without strong positive or negative tendencies.
        </li>
        <li style="font-size: 20px;">
          Positive Emotions: Reflects motivating positive experiences such as joy and fulfillment.
        </li>
        <li style="font-size: 20px;">
          Concentration & Thinking: A focused allocation of cognitive resources toward a specific task or goal.
        </li>
        <li style="font-size: 20px;">
          Surprise & Confusion: Transient emotions elicited by sudden stimuli, ranging from curiosity to alertness.
        </li>
        <li style="font-size: 20px;">
          Negative Emotions: Encompasses sadness, anger, and other typically suppressed negative emotional states.
        </li>
        <li style="font-size: 20px;">
          Physiological emotions: Refer to bodily states like fatigue or pain that arise from direct biological feedback.
        <li style="font-size: 20px;">
          Miscellaneous: Labeling uncertainty in mood determination due to incomplete information or ambiguity in expression.
        </li>
      </ul>
      -->

      <h3 class="title is-4">
        State Statistic
      </h3>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        We labeled the <strong style="color: #f7a049;">FiT-AG</strong> dataset and the <strong style="color: #b8aeea;">FiT-PVSG</strong> dataset with <strong style="color: #f7a049;">115</strong> and <strong style="color: #b8aeea;">125</strong> categories of state labels, respectively.  Based on object state characteristics, we categorized state labels into nine categories: 
      </p>
      <figure class="image" style="margin-bottom: 1.5rem;">
        <img src="static/images/fig_state_distribution.png" alt="fig_state" style="max-width: 100%; height: auto;">
      </figure>
      <!--
      <ul style="margin-left: 1.5em; list-style-type: disc; margin-top: 1.5rem; margin-bottom: 1.5rem;">
        <li style="font-size: 20px;">
          Functional state: Describes the real-time operational mode of a system, device, or object.
        </li>
        <li style="font-size: 20px;">
          Functional state： Describes the real-time operational mode of a system, device, or object.
        </li>
        <li style="font-size: 20px;">
          Cleanliness & Maintenance: Assesses the hygiene and upkeep of a place or object.
        </li>
        <li style="font-size: 20px;">
          Usage & Consumption: Refers to the states of items regarding their preparation, condition, or depletion.
        </li>
        <li style="font-size: 20px;">
          Spatial State: Describes an object’s position, orientation, motion, and interaction within space.
        </li>
        <li style="font-size: 20px;">
          Visual Property: Refers to the visual attributes of an object or scene, including material, color, surface condition, environment, and effects, used to accurately describe its appearance.
        </li>
        <li style="font-size: 20px;">
          Dynamic State: Describes an object or system in a constantly changing, active state.
        </li>
        <li style="font-size: 20px;">
          Biological & Environmental: Refer to an individual’s internal biological condition and external natural and social surroundings.
        </li>
        <li style="font-size: 20px;">
          Uncertainty state: Refers to missing or incomplete information in the system, reflecting uncertainty about the current situation or data reliability.
        </li>
      </ul>
      -->


      <h2 class="title is-3" style="text-align: center;">
        Method and Experimental
      </h2>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        We propose a model-agnostic module named <strong style="color: #FFC000;">Text-Augmented Visual Semantic (TAVS)</strong> module that integrates fine-grained text cues—<strong style="color: #99CC00;">emotion</strong>, <strong style="color: #4BACC6;">state</strong>, and <strong style="color: #C05046;">frame-level caption</strong> encoded by CLIP—into baseline models to enhance reasoning about complex object interactions and narrative dynamics.
      </p>
      <figure class="image" style="margin: 0 auto 1rem auto; text-align: center; width: fit-content;">
        <img src="static/images/fig_method_experiment.png" alt="fig_method_experiment" style="max-width: 100%; height: auto;">
      </figure>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        Different baselines incorporate TAVS modules with different boosts on the fitbench.
      </p>
      <div class="highlight-box" >
          <b>Takeaway I:</b> Different baselines incorporate TAVS modules with different boosts on the fitbench.
      </div>
      <div class="highlight-box" >
          <b>Takeaway II:</b> When observing a limited portion of the video, the improvement is notably higher, suggesting that fine-grained text cues offer strong priors for future reasoning.
      </div>
      <div class="highlight-box" >
          <b>Takeaway III:</b> The mean recall results suggest that the model struggles to generalize across the diverse relation types in <strong style="color: #b8aeea;">FiT-PVSG</strong>, reflecting the increased difficulty of scene graph prediction on this dataset.
      </div>

      <h2 class="title is-3" style="text-align: center;">
        Case Study
      </h2>
      <p style="margin-top: 1.5rem; margin-bottom: 1.5rem;font-size: 20px;">
        Scene graph anticipation results of baseline(SceneSayerSDE) and baseline(SceneSayerSDE) with <strong style="color: #FFC000;">TAVS</strong> module on the proposed FiTBench. We report performance for both short-term and long-term future predictions.
      </p>
      <figure class="image" style="margin: 0 auto 1rem auto; text-align: center; width: fit-content;">
        <img src="static/images/fig_visualize_supp.png" alt="fig_visualize" style="max-width: 100%; height: auto;">
      </figure>
      <div class="highlight-box" >
          <b>Takeaway IV:</b> Our approach performs better in both short-term and long-term predictions.
      </div>

    </div>
  </div>
</section>

 
   <!-- Next Steps / Future Directions Section -->
  <section class="section">
    
    <h2 class="title is-3" style="text-align: center;">
      Conclusion
    </h2>
  
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
  
            <div class="finding-box mb-4">
              <span style="font-size: larger; text-align: center; font-weight: bold;">
                In this work, we propose a new benchmark VinaBench, which introduces fine-grained contextual drivers through a systematic annotation pipeline to facilitate understanding of the narrative reasons for relation evolution. 
            </div>
            <div class="finding-box mb-4">
              <span style="font-size: larger; text-align: center; font-weight: bold;">
                FiTBench incorporates ego-centric videos featuring rare hand-object interactions, which differ from the common exo-centric views. This shift in perspective presents new challenges for SGA.
              </span>
            </div>
            <div class="finding-box mb-4">
              <span style="font-size: larger; text-align: center; font-weight: bold;">
                A model-agnostic Text-Augmented Visual Semantics (TAVS) module is designed to reason about narrative evolution by incorporating the fine-grained text cues.
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  

 

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
